# Plan A 训练结果分析报告

## 执行信息
- **训练脚本**: `examples/train_a5000.py`
- **配置文件**: `configs/ppo/ppo_trajectory_filtering_2v2_a5000_planA.yaml`
- **训练时长**: 约5.5小时（19995秒）
- **总更新次数**: 10000
- **总步数**: 5,690,499

---

## 一、关键指标变化趋势

### 1.1 训练奖励（Episode Reward）

| 更新次数 | Episode Reward | Episode Length |
|---------|----------------|----------------|
| 10      | -40.90         | 1200           |
| 100     | -85.40         | 1200           |
| 500     | -85.17         | ~1200          |
| 1000    | -118.30        | ~1200          |
| 2000    | -26.16         | ~1200          |
| 5000    | -57.67         | ~1200          |
| 10000   | -88.59         | 996            |

**问题**：
- ❌ 训练奖励**持续下降**，从-40.90恶化到-88.59
- ❌ 没有学习到有效策略，奖励反而越来越差
- ⚠️ Episode长度略有下降（1200→996），说明episode提前结束

### 1.2 评估奖励（Eval Mean Reward）

| 更新次数 | Eval Reward | Win Rate | Draw Rate |
|---------|-------------|----------|-----------|
| 50      | -1.20       | 0%       | 100%      |
| 100     | -121.20     | 0%       | 100%      |
| 4900    | -120.90     | 蓝队100% | 0%        |
| 10000   | -120.40     | 红队100% | 0%        |

**严重问题**：
- ❌ 评估奖励**固定且极低**（-120左右），几乎没有变化
- ❌ 评估时策略**完全固定**：早期100%平局，后期100%某一队获胜
- ❌ 说明策略**没有学习到有效的对抗策略**，只是学会了"站着不动"或"固定模式"

### 1.3 价值函数学习（Value Loss & Explained Variance）

| 更新次数 | Value Loss | VF Explained Var |
|---------|------------|------------------|
| 10      | 0.207      | -                |
| 100     | 0.974      | -                |
| 1000    | 1.678      | -                |
| 2000    | 2.314      | -                |
| 10000   | 1.469      | **-0.367**      |

**严重问题**：
- ❌ Value Loss**持续上升**（0.207 → 1.469），价值函数学习失败
- ❌ **Explained Variance为负值（-0.367）**，说明价值函数预测能力比随机猜测还差
- ❌ 价值函数无法准确估计状态价值，导致策略学习失败

### 1.4 探索能力（Entropy）

| 更新次数 | Entropy | 趋势 |
|---------|---------|------|
| 10      | 3.043    | 初始 |
| 100     | 2.467    | ↓    |
| 1000    | 1.743    | ↓    |
| 2000    | 2.435    | ↑    |
| 5000    | 1.774    | ↓    |
| 10000   | 1.905    | ↓    |

**问题**：
- ⚠️ Entropy**持续下降**（3.04 → 1.90），探索能力减弱
- ⚠️ 虽然Plan A将entropy_coef从0.01增加到0.05，但探索仍然不足
- ⚠️ 策略过早收敛到固定模式

### 1.5 策略损失（Policy Loss）

| 更新次数 | Policy Loss | 特征 |
|---------|-------------|------|
| 10      | 0.431       | 正常 |
| 100     | 0.078       | 下降 |
| 1000    | -0.005      | 负值 |
| 2000    | -0.430      | 极端负值 |
| 5000    | 0.484       | 波动 |
| 10000   | 0.242       | 不稳定 |

**问题**：
- ⚠️ Policy Loss**波动极大**，出现极端负值（-2.18, -1.70）
- ⚠️ 说明策略更新**不稳定**，可能存在梯度爆炸或策略更新过大

---

## 二、根本原因分析

### 2.1 价值函数学习失败（最严重）

**现象**：
- Value Loss持续上升（0.2 → 1.5）
- Explained Variance为负（-0.367）
- 价值函数无法准确估计状态价值

**原因**：
1. **价值函数容量不足**：虽然Plan A增加了value_head hidden_dims到[256, 128]，但可能仍然不够
2. **价值函数裁剪过强**：`vf_clip_param: 10.0`可能限制了价值函数的学习
3. **奖励设计问题**：step_reward=-0.001，dead_penalty=-0.1，奖励信号可能不够清晰
4. **轨迹过滤影响**：filter_ratio=0.5可能过滤掉了重要的价值学习样本

**影响**：
- 价值函数无法提供准确的优势估计
- 策略更新方向错误
- 导致策略学习失败

### 2.2 奖励设计问题

**当前奖励设计**：
- `step_reward: -0.001`（每步惩罚）
- `dead_penalty: -0.1`（死亡惩罚）
- `attack_penalty: -0.1`（攻击惩罚）
- `attack_opponent_reward: 0.2`（攻击敌人奖励）

**问题**：
1. **奖励稀疏**：只有攻击敌人时才有正奖励（0.2），其他都是惩罚
2. **惩罚过重**：每步-0.001，300步就是-0.3，远大于攻击奖励0.2
3. **奖励不平衡**：攻击奖励（0.2）vs 攻击惩罚（-0.1），净收益只有0.1，但需要承担死亡风险
4. **没有获胜奖励**：获胜时没有额外奖励，导致策略倾向于"不输"而非"获胜"

**影响**：
- 策略倾向于"站着不动"或"避免战斗"
- 评估时100%平局或固定模式
- 无法学习到有效的对抗策略

### 2.3 轨迹过滤过于激进

**当前配置**：
- `filter_ratio: 0.5`（保留50%的轨迹）
- `filter_strategy: "mixed"`（混合策略）

**问题**：
1. **过滤比例过高**：虽然Plan A从0.75降低到0.5，但50%的过滤仍然可能过滤掉重要的学习样本
2. **价值函数样本不足**：价值函数需要大量样本来学习，过滤后样本可能不足
3. **过滤策略可能偏向**：mixed策略可能偏向某些类型的轨迹，导致学习偏差

**影响**：
- 价值函数学习样本不足
- 策略学习样本不足
- 导致学习不稳定

### 2.4 探索不足

**当前配置**：
- `entropy_coef: 0.05`（Plan A从0.01增加到0.05）

**问题**：
1. **Entropy仍然下降**：虽然增加了entropy_coef，但entropy仍然从3.04下降到1.90
2. **探索不足**：策略过早收敛到固定模式
3. **自博弈对手固定**：self_play_update_freq=5，对手更新频率可能不够

**影响**：
- 策略过早收敛
- 无法探索到更好的策略
- 评估时策略固定

### 2.5 训练不稳定

**现象**：
- Policy Loss波动极大（-2.18到0.48）
- Value Loss波动极大（0.68到10.31）
- Episode Reward波动极大（-118到4.40）

**原因**：
1. **学习率可能过高**：lr=3e-4可能对于2v2环境过高
2. **批次大小过大**：batch_size=5120可能过大，导致更新不稳定
3. **训练轮数过多**：num_epochs=5可能过多，导致过拟合
4. **梯度裁剪不足**：max_grad_norm=0.5可能不够

**影响**：
- 训练不稳定
- 策略更新过大
- 导致学习失败

---

## 三、解决方案

### 3.1 立即修复（高优先级）

#### 3.1.1 修复价值函数学习

**方案**：
1. **增加价值函数容量**：
   ```yaml
   value_head:
     params:
       hidden_dims: [512, 256, 128]  # 从[256, 128]增加到[512, 256, 128]
   ```

2. **降低价值函数裁剪**：
   ```yaml
   vf_clip_param: 5.0  # 从10.0降低到5.0
   ```

3. **增加价值函数系数**：
   ```yaml
   value_coef: 1.0  # 从0.5增加到1.0
   ```

4. **降低轨迹过滤比例**：
   ```yaml
   filter_ratio: 0.3  # 从0.5降低到0.3，保留更多样本
   ```

#### 3.1.2 修复奖励设计

**方案**：
1. **移除每步惩罚**：
   ```yaml
   step_reward: 0.0  # 从-0.001改为0.0
   ```

2. **增加获胜奖励**：
   ```yaml
   win_reward: 10.0  # 获胜时额外奖励
   lose_penalty: -5.0  # 失败时额外惩罚
   ```

3. **调整攻击奖励**：
   ```yaml
   attack_opponent_reward: 1.0  # 从0.2增加到1.0
   attack_penalty: 0.0  # 从-0.1改为0.0
   ```

#### 3.1.3 增加探索

**方案**：
1. **进一步增加熵系数**：
   ```yaml
   entropy_coef: 0.1  # 从0.05增加到0.1
   ```

2. **增加对手更新频率**：
   ```yaml
   self_play_update_freq: 3  # 从5降低到3，更频繁更新对手
   ```

3. **增加策略池大小**：
   ```yaml
   policy_pool_size: 100  # 从50增加到100
   ```

#### 3.1.4 稳定训练

**方案**：
1. **降低学习率**：
   ```yaml
   lr: 1e-4  # 从3e-4降低到1e-4
   ```

2. **降低批次大小**：
   ```yaml
   batch_size: 2560  # 从5120降低到2560
   ```

3. **降低训练轮数**：
   ```yaml
   num_epochs: 3  # 从5降低到3
   ```

4. **增加梯度裁剪**：
   ```yaml
   max_grad_norm: 0.3  # 从0.5降低到0.3
   ```

### 3.2 中期优化（中优先级）

#### 3.2.1 改进轨迹过滤策略

**方案**：
1. **使用更保守的过滤策略**：
   ```yaml
   filter_strategy: "advantage"  # 从"mixed"改为"advantage"，更简单直接
   filter_ratio: 0.3  # 进一步降低过滤比例
   ```

2. **禁用轨迹过滤（用于对比）**：
   ```yaml
   use_trajectory_filter: false  # 先禁用，验证是否是过滤导致的问题
   ```

#### 3.2.2 改进自博弈策略

**方案**：
1. **使用更激进的对手采样**：
   ```yaml
   opponent_pool_strategy: "uniform"  # 从"pfsp"改为"uniform"，更随机
   ```

2. **增加对手多样性**：
   ```yaml
   policy_pool_size: 100
   opponent_sampling_temperature: 1.5  # 增加采样温度
   ```

### 3.3 长期优化（低优先级）

#### 3.3.1 网络架构优化

**方案**：
1. **增加编码器容量**：
   ```yaml
   encoder:
     params:
       hidden_dims: [512, 512, 256]  # 从[512, 256, 128]增加到[512, 512, 256]
   ```

2. **使用残差连接**：
   ```yaml
   encoder:
     params:
       use_residual: true  # 启用残差连接
   ```

#### 3.3.2 学习率调度优化

**方案**：
1. **使用余弦退火**：
   ```yaml
   optimizer:
     params:
       scheduler: "cosine"  # 从"linear"改为"cosine"
   ```

2. **增加warmup步数**：
   ```yaml
   warmup_steps: 5000  # 从2000增加到5000
   ```

---

## 四、预期改进效果

### 4.1 短期目标（1000更新内）

- ✅ 训练奖励从-88提升到-50以内
- ✅ 评估奖励从-120提升到-80以内
- ✅ Value Loss从1.5降低到0.5以内
- ✅ Explained Variance从-0.37提升到0.3以上
- ✅ Entropy维持在2.0以上

### 4.2 中期目标（5000更新内）

- ✅ 训练奖励提升到0以上（开始获得正奖励）
- ✅ 评估奖励提升到-50以内
- ✅ 评估时出现胜负（win_rate > 0且 < 1）
- ✅ Value Loss稳定在0.3以内
- ✅ Explained Variance稳定在0.5以上

### 4.3 长期目标（10000更新内）

- ✅ 训练奖励稳定在10以上
- ✅ 评估奖励稳定在0以上
- ✅ 评估时win_rate在0.3-0.7之间（有竞争性）
- ✅ Value Loss稳定在0.2以内
- ✅ Explained Variance稳定在0.7以上

---

## 五、建议的修复优先级

### 优先级1（必须立即修复）
1. **修复价值函数学习**（增加容量、降低裁剪、增加系数）
2. **修复奖励设计**（移除每步惩罚、增加获胜奖励）
3. **降低轨迹过滤比例**（从0.5降低到0.3）

### 优先级2（尽快修复）
4. **增加探索**（增加entropy_coef到0.1）
5. **稳定训练**（降低学习率、批次大小、训练轮数）

### 优先级3（可以尝试）
6. **改进轨迹过滤策略**（使用advantage策略或禁用）
7. **改进自博弈策略**（使用uniform采样）

---

## 六、总结

### 6.1 主要问题
1. ❌ **价值函数学习完全失败**（最严重）
2. ❌ **奖励设计不合理**（导致策略倾向于"不输"而非"获胜"）
3. ❌ **轨迹过滤过于激进**（导致学习样本不足）
4. ⚠️ **探索不足**（策略过早收敛）
5. ⚠️ **训练不稳定**（损失波动极大）

### 6.2 核心原因
**价值函数学习失败是核心问题**，导致：
- 无法准确估计状态价值
- 策略更新方向错误
- 策略学习失败

**奖励设计不合理是次要问题**，导致：
- 策略倾向于"站着不动"
- 无法学习到有效的对抗策略

### 6.3 修复方向
1. **优先修复价值函数学习**（增加容量、降低裁剪、增加系数）
2. **修复奖励设计**（移除每步惩罚、增加获胜奖励）
3. **降低轨迹过滤比例**（保留更多学习样本）
4. **增加探索**（增加entropy_coef）
5. **稳定训练**（降低学习率、批次大小）

### 6.4 预期效果
修复后，预期在1000更新内：
- 训练奖励从-88提升到-50以内
- 评估奖励从-120提升到-80以内
- Value Loss从1.5降低到0.5以内
- Explained Variance从-0.37提升到0.3以上

---

## 附录：关键数据点

### A.1 训练指标变化

```
Update 10:   Reward=-40.90, ValueLoss=0.207, Entropy=3.043
Update 100:  Reward=-85.40, ValueLoss=0.974, Entropy=2.467
Update 500:  Reward=-85.17, ValueLoss=0.838, Entropy=2.290
Update 1000: Reward=-118.30, ValueLoss=1.678, Entropy=1.743
Update 2000: Reward=-26.16, ValueLoss=2.314, Entropy=2.435
Update 5000: Reward=-57.67, ValueLoss=2.023, Entropy=1.774
Update 10000: Reward=-88.59, ValueLoss=1.469, Entropy=1.905
```

### A.2 评估指标变化

```
Update 50:   EvalReward=-1.20,   DrawRate=100%, WinRate=0%
Update 100:  EvalReward=-121.20, DrawRate=100%, WinRate=0%
Update 4900: EvalReward=-120.90, DrawRate=0%,   WinRate=100% (蓝队)
Update 10000: EvalReward=-120.40, DrawRate=0%,   WinRate=100% (红队)
```

### A.3 价值函数指标

```
Update 10000: ValueLoss=1.469, VFExplainedVar=-0.367
```

**说明**：Explained Variance为负值，说明价值函数预测能力比随机猜测还差。

---

**报告生成时间**: 2025-11-15
**分析者**: AI Assistant
**报告版本**: 1.0

