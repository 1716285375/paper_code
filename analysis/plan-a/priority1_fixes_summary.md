# 优先级1修复总结

## 修复时间
2025-11-15

## 修复内容

### 1. 修复价值函数学习（最严重问题）

#### 1.1 增加价值函数容量
- **修改前**: `hidden_dims: [256, 128]`
- **修改后**: `hidden_dims: [512, 256, 128]`
- **原因**: 价值函数容量不足导致学习失败，增加容量可以提升价值函数的学习能力
- **预期效果**: Value Loss从1.5降低到0.5以内，Explained Variance从-0.37提升到0.3以上

#### 1.2 降低价值函数裁剪
- **修改前**: `vf_clip_param: 10.0`
- **修改后**: `vf_clip_param: 5.0`
- **原因**: 价值函数裁剪过强限制了价值函数的学习，降低裁剪允许价值函数学习更大的价值范围
- **预期效果**: 价值函数能够学习更准确的状态价值估计

#### 1.3 增加价值函数系数
- **修改前**: `value_coef: 0.5`
- **修改后**: `value_coef: 1.0`
- **原因**: 增加价值函数学习权重，提升价值函数在总损失中的重要性
- **预期效果**: 价值函数学习更加稳定和有效

### 2. 修复奖励设计

#### 2.1 移除每步惩罚
- **修改前**: `step_reward: -0.001`
- **修改后**: `step_reward: 0.0`
- **原因**: 每步惩罚导致策略倾向于"站着不动"，300步就是-0.3，远大于攻击奖励
- **预期效果**: 策略不再倾向于避免行动，能够学习到积极的对抗策略

#### 2.2 移除攻击惩罚
- **修改前**: `attack_penalty: -0.1`
- **修改后**: `attack_penalty: 0.0`
- **原因**: 攻击惩罚与攻击奖励矛盾，导致净收益只有0.1，但需要承担死亡风险
- **预期效果**: 鼓励攻击行为，策略能够学习到有效的战斗策略

#### 2.3 大幅增加攻击奖励
- **修改前**: `attack_opponent_reward: 0.2`
- **修改后**: `attack_opponent_reward: 1.0`
- **原因**: 攻击奖励过低，无法激励策略进行积极的战斗
- **预期效果**: 策略能够学习到积极的攻击行为，评估时出现胜负而非100%平局

#### 2.4 获胜奖励（待实现）
- **状态**: 未实现（MAgent2环境可能不支持）
- **原因**: 需要在环境包装器中实现获胜/失败奖励
- **建议**: 如果评估时仍然出现100%平局，考虑在环境包装器中添加获胜奖励

### 3. 降低轨迹过滤比例

#### 3.1 降低过滤比例
- **修改前**: `filter_ratio: 0.5`
- **修改后**: `filter_ratio: 0.3`
- **原因**: 过滤比例过高导致价值函数学习样本不足，降低过滤比例保留更多训练数据
- **预期效果**: 价值函数和策略都有更多的学习样本，学习更加稳定

---

## 修复前后对比

### 价值函数相关
| 参数 | 修复前 | 修复后 | 变化 |
|------|--------|--------|------|
| value_head hidden_dims | [256, 128] | [512, 256, 128] | 容量增加 |
| vf_clip_param | 10.0 | 5.0 | 裁剪降低 |
| value_coef | 0.5 | 1.0 | 权重增加 |

### 奖励设计
| 参数 | 修复前 | 修复后 | 变化 |
|------|--------|--------|------|
| step_reward | -0.001 | 0.0 | 移除惩罚 |
| attack_penalty | -0.1 | 0.0 | 移除惩罚 |
| attack_opponent_reward | 0.2 | 1.0 | 奖励增加5倍 |

### 轨迹过滤
| 参数 | 修复前 | 修复后 | 变化 |
|------|--------|--------|------|
| filter_ratio | 0.5 | 0.3 | 过滤降低40% |

---

## 预期改进效果

**说明**：总训练更新次数为10000，以下时间点是指在训练的前N个更新内应该达到的目标。

### 短期目标（前1000个更新内，即前10%训练时间）
- ✅ 训练奖励从-88提升到-50以内
- ✅ 评估奖励从-120提升到-80以内
- ✅ Value Loss从1.5降低到0.5以内
- ✅ Explained Variance从-0.37提升到0.3以上
- ✅ 评估时出现胜负（win_rate > 0且 < 1）

### 中期目标（前5000个更新内，即前50%训练时间）
- ✅ 训练奖励提升到0以上（开始获得正奖励）
- ✅ 评估奖励提升到-50以内
- ✅ Value Loss稳定在0.3以内
- ✅ Explained Variance稳定在0.5以上

---

## 注意事项

1. **价值函数容量增加**：从[256, 128]增加到[512, 256, 128]会增加模型参数量，但A5000 GPU（24GB显存）应该能够支持

2. **奖励设计变化**：移除每步惩罚和攻击惩罚，大幅增加攻击奖励，可能导致训练初期奖励波动较大，这是正常的

3. **轨迹过滤降低**：从0.5降低到0.3，保留更多样本，训练时间可能会略微增加，但学习效果应该更好

4. **获胜奖励**：如果评估时仍然出现100%平局，考虑在环境包装器中实现获胜/失败奖励机制

---

## 下一步建议

如果修复后仍然存在问题，建议：

1. **进一步降低轨迹过滤比例**：从0.3降低到0.2或完全禁用
2. **增加探索**：将entropy_coef从0.05增加到0.1
3. **稳定训练**：降低学习率、批次大小、训练轮数
4. **实现获胜奖励**：在环境包装器中添加获胜/失败奖励机制

---

**修复完成时间**: 2025-11-15
**修复者**: AI Assistant
**配置文件**: `configs/ppo/ppo_trajectory_filtering_2v2_a5000_planA.yaml`

