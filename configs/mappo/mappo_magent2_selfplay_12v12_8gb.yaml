# MAPPO自博弈训练配置 - 12v12 Magent2环境，8GB显存优化
# 针对RTX 4060 Ti (8GB) 优化，使用集中式Critic

# 环境配置
env:
  name: "magent2:battle_v4"
  map_size: 20  # 12v12使用20x20地图
  max_cycles: 100  # Episode最大步数
  minimap_mode: false

# 训练配置
training:
  num_updates: 2000  # 训练更新次数
  batch_size: 128  # 8GB显存限制下的batch size
  num_epochs: 3  # 每批数据训练轮数
  max_steps_per_episode: 100  # Episode最大步数
  
  # PPO核心参数
  clip_coef: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  vf_clip_param: 10.0  # 价值函数裁剪参数
  
  # GAE参数
  gamma: 0.99
  gae_lambda: 0.95
  
  # 学习率
  lr: 3e-4
  
  # 梯度裁剪
  max_grad_norm: 0.5
  
  # MAPPO特定参数
  use_centralized_critic: true  # 使用集中式Critic
  global_obs_dim: 2000  # Magent2全局状态维度（20*20*5=2000）
  opp_action_in_cc: false  # 是否在集中式Critic中使用对手动作
  
  # 自博弈参数
  self_play_update_freq: 10
  self_play_mode: "pool"
  use_policy_pool: true
  policy_pool_size: 15
  opponent_pool_strategy: "pfsp"
  
  # 评估和保存
  eval_freq: 50
  save_freq: 100
  log_freq: 10

# Agent配置
agent:
  type: "ppo"
  obs_dim: 845
  action_dim: 21
  
  # 编码器（减小网络规模）
  encoder:
    type: "networks/mlp"
    params:
      in_dim: 845
      hidden_dims: [128, 64]  # 减小网络规模
      use_layer_norm: true
      dropout: 0.0
  
  # 策略头
  policy_head:
    type: "policy_heads/discrete"
    params:
      hidden_dims: [32]
  
  # 价值头（局部Critic）
  value_head:
    type: "value_heads/mlp"
    params:
      hidden_dims: [32]
  
  # 集中式Critic配置
  centralized_critic:
    state_dim: 2000  # Magent2全局状态维度
    hidden_dims: [128, 64]  # 集中式Critic网络规模
    use_opponent_actions: false  # 不使用对手动作
    opponent_action_dim: 21  # 如果使用对手动作，动作维度
    n_opponents: 12  # 对手数量（12v12）
  
  # 优化器
  optimizer:
    type: "optimizers/adam"
    params:
      lr: 3e-4
      weight_decay: 0.0

# 其他配置
seed: 42
device: "cuda"

# 数据保存配置
data_saving:
  enabled: true
  format: "json"
  save_freq: 10

# 跟踪配置
tracking:
  enabled: true
  type: "tensorboard"
  project: "marl-selfplay"

