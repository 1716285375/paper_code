# PPO自博弈训练配置 - 12v12 Magent2环境，8GB显存优化
# 针对RTX 4060 Ti (8GB) 优化

# 环境配置
env:
  name: "magent2:battle_v4"
  map_size: 20  # 12v12使用20x20地图
  max_cycles: 100  # Episode最大步数
  minimap_mode: false

# 训练配置
training:
  num_updates: 2000  # 训练更新次数
  batch_size: 128  # 8GB显存限制下的batch size
  num_epochs: 3  # 每批数据训练轮数
  max_steps_per_episode: 100  # Episode最大步数
  
  # PPO核心参数
  clip_coef: 0.2  # PPO裁剪系数
  value_coef: 0.5  # 价值损失权重
  entropy_coef: 0.01  # 熵正则化权重
  vf_clip_param: 10.0  # 价值函数裁剪参数（可选）
  
  # GAE参数
  gamma: 0.99  # 折扣因子
  gae_lambda: 0.95  # GAE lambda参数
  
  # 学习率
  lr: 3e-4
  
  # 梯度裁剪
  max_grad_norm: 0.5
  
  # 自博弈参数
  self_play_update_freq: 10  # 每10个更新更新一次对手策略
  self_play_mode: "pool"  # "copy" 或 "pool"
  use_policy_pool: true
  policy_pool_size: 15
  opponent_pool_strategy: "pfsp"  # "uniform", "elo", "pfsp"
  
  # 评估和保存
  eval_freq: 50  # 每50个更新评估一次
  save_freq: 100  # 每100个更新保存一次
  log_freq: 10  # 每10个更新记录一次日志

# Agent配置
agent:
  type: "ppo"
  obs_dim: 845  # Magent2观测维度
  action_dim: 21  # Magent2动作维度
  
  # 编码器（减小网络规模以适配8GB显存）
  encoder:
    type: "networks/mlp"
    params:
      in_dim: 845
      hidden_dims: [128, 64]  # 减小网络规模
      use_layer_norm: true
      dropout: 0.0
  
  # 策略头
  policy_head:
    type: "policy_heads/discrete"
    params:
      hidden_dims: [32]  # 减小策略头规模
  
  # 价值头
  value_head:
    type: "value_heads/mlp"
    params:
      hidden_dims: [32]  # 减小价值头规模
  
  # 优化器
  optimizer:
    type: "optimizers/adam"
    params:
      lr: 3e-4
      weight_decay: 0.0

# 其他配置
seed: 42
device: "cuda"

# 数据保存配置
data_saving:
  enabled: true
  format: "json"
  save_freq: 10

# 跟踪配置（可选）
tracking:
  enabled: true
  type: "tensorboard"  # "tensorboard" 或 "wandb"
  project: "marl-selfplay"

