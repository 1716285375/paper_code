# HP3O自博弈训练配置 - 2v2 Magent2环境，6000轮训练
# HP3O (High-Performance PPO with Trajectory Replay) 算法
# 针对2v2小规模对战优化，训练红方策略，超长时间训练版本

# 环境配置
env:
  name: "magent2:battle_v4"
  map_size: 12  # 2v2使用12x12地图
  max_cycles: 500  # Episode最大步数（更长的episode）
  minimap_mode: false

# 训练配置
training:
  num_updates: 6000  # 训练更新次数（超长时间训练）
  batch_size: 128  # 2v2规模较小，可以使用更大的batch size
  num_epochs: 10  # 每批数据训练轮数
  max_steps_per_episode: 500  # Episode最大步数（与max_cycles一致）
  
  # PPO核心参数
  clip_coef: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  vf_clip_param: 10.0  # 价值函数裁剪参数
  
  # GAE参数
  gamma: 0.99
  gae_lambda: 0.95
  
  # 学习率
  lr: 3e-4
  
  # 梯度裁剪
  max_grad_norm: 0.5
  
  # HP3O特定参数（轨迹重放）
  trajectory_buffer_size: 10  # 轨迹缓冲区大小（存储的轨迹数量）
  trajectory_sample_size: 2  # 每次采样轨迹数量（2v2场景降低到2，减少初期警告）
  data_sample_size: 256  # 从轨迹中采样数据量
  threshold: 0.5  # 轨迹筛选阈值（用于use_best_value）
  use_best_value: false  # 是否使用最佳轨迹的价值函数（设置为true可启用价值函数增强）
  
  # 自博弈参数
  self_play_update_freq: 10  # 每N个更新更新一次对手策略
  self_play_mode: "pool"  # 自博弈模式：copy（直接复制）或 pool（策略池采样）
  use_policy_pool: true  # 是否使用策略池
  policy_pool_size: 15  # 策略池大小
  opponent_pool_strategy: "pfsp"  # 策略池采样策略：uniform, elo, pfsp
  
  # 评估和保存
  eval_freq: 50  # 评估频率（适应6000轮训练）
  save_freq: 100  # 保存频率（每100轮保存一次）
  log_freq: 10  # 日志记录频率

# Agent配置
agent:
  type: "ppo"  # 使用标准PPO Agent
  obs_dim: 845  # Magent2观测维度（与map_size无关，保持845）
  action_dim: 21  # Magent2动作维度
  
  # 编码器（2v2规模，可以使用稍大的网络）
  encoder:
    type: "networks/mlp"  # 使用完整路径：networks/mlp, networks/cnn, networks/lstm, networks/gru
    params:
      in_dim: 845
      hidden_dims: [256, 128]  # 2v2可以使用稍大的网络
      use_layer_norm: true
      dropout: 0.0
  
  # Policy Head
  policy_head:
    type: "policy_heads/discrete"  # 离散动作空间
    params:
      hidden_dims: [64]  # 稍大的策略头
  
  # Value Head
  value_head:
    type: "value_heads/mlp"  # MLP价值头
    params:
      hidden_dims: [64]  # 稍大的价值头
  
  # 优化器配置
  optimizer:
    type: "optimizers/adam"  # Adam优化器
    params:
      lr: 3e-4
      weight_decay: 0.0
      max_grad_norm: 0.5
      scheduler: "linear"  # 线性学习率调度
      warmup_steps: 600  # warmup步数（适应6000轮训练）
      total_steps: 600000  # 总步数（6000 updates * 100 steps per update估算）

# 数据保存配置
data_saving:
  enabled: true
  format: "json"  # json, csv, both
  save_freq: 10  # 每N个更新保存一次

# 日志和追踪配置
logging:
  log_dir: "logs"
  log_to_file: true
  log_to_console: true

tracking:
  enabled: true
  type: "tensorboard"
  project: "marl-selfplay-2v2"

