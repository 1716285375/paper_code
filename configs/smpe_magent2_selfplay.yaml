# SMPE²自博弈训练配置
# 结合VAE状态建模、Filter过滤、SimHash内在奖励与自博弈对手池

# 环境配置
env:
  id: "magent2:battle_v4"
  kwargs:
    map_size: 45
    max_cycles: 1000
    minimap_mode: false

# Agent配置
agent:
  type: "smpe"  # SMPE² Agent
  obs_dim: 845
  action_dim: 21
  agent_id_dim: 32  # Agent ID嵌入维度
  
  # 基础PPO配置
  encoder:
    type: "networks/mlp"
    params:
      in_dim: 845
      hidden_dims: [512, 256, 128]
      use_layer_norm: true
      dropout: 0.0
  
  policy_head:
    type: "policy_heads/discrete"
    params:
      hidden_dims: [64]
  
  value_head:
    type: "value_heads/mlp"
    params:
      hidden_dims: [64]
  
  optimizer:
    type: "optimizers/adam"
    params:
      lr: 3e-4
      scheduler: "linear"
      warmup_steps: 1000
      total_steps: 100000
      weight_decay: 0.0
      max_grad_norm: 0.5
  
  # SMPE²模块配置
  use_vae: true  # 启用VAE状态建模
  use_filter: true  # 启用Filter过滤
  use_intrinsic: true  # 启用SimHash内在奖励
  
  # VAE配置
  vae:
    z_dim: 16  # 潜在变量维度
    encoder_hidden_dims: [64, 32]
    decoder_hidden_dims: [32, 64]
    beta_kl: 1e-3  # KL散度权重
    rec_coef: 1.0  # 观测重构权重
    action_rec_coef: 1.5  # 动作重构权重
  
  # Filter配置
  filter:
    num_filters: 8  # 过滤器数量
    use_gumbel: false  # 使用Sigmoid而非Gumbel
    temperature: 0.5
    tau: 0.01  # 目标网络软更新系数
  
  # SimHash内在奖励配置
  intrinsic_reward:
    hash_bits: 512
    bucket_size: 65536  # 2^16
    r_max: 0.2
    normalize: true

# 训练配置
training:
  num_updates: 10000
  max_steps_per_episode: 500
  num_epochs: 4
  batch_size: 2048
  clip_coef: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  gamma: 0.99
  gae_lambda: 0.95
  
  # SMPE²特定配置
  vae_update_freq: 1024  # 每1024环境步更新VAE
  vae_epochs: 3  # VAE训练轮数
  filter_update_freq: 1  # Filter更新频率（每步）
  
  # 组合奖励权重
  intrinsic_reward_beta1: 0.1  # SimHash内在奖励权重（可调整0.1-0.3）
  intrinsic_reward_beta2: 0.05  # 自博弈奖励权重（可调整0.05-0.2）
  intrinsic_warmup_steps: 20000  # 内在奖励warm-up步数
  
  # 自博弈配置
  self_play_update_freq: 10  # 每10个更新更新对手策略
  main_team: "team_red"
  opponent_team: "team_blue"
  
  # 对手池配置
  opponent_pool:
    strategy: "pfsp"  # "uniform", "elo", "pfsp"
    size: 15  # 对手池大小
  snapshot_freq: 50000  # 每50000环境步快照对手策略
  
  # 评估和保存
  eval_freq: 50
  save_freq: 100
  log_freq: 10

# 设备配置
device: "cuda"  # "cuda" 或 "cpu"

# 随机种子
seed: 42

# 实验跟踪配置
tracking:
  enabled: true
  wandb:
    enabled: true
    project: "magent2-smpe-selfplay"
    name: null  # 自动生成：环境名_时间戳
  tensorboard:
    enabled: true
    project: "magent2-smpe-selfplay"
    name: "smpe_selfplay"

# 数据保存配置
data_saving:
  enabled: true
  output_dir: "training_data"
  format: "both"  # "json", "csv", "both"
  record_video: false  # 是否录制评估视频

