# SMPE²自博弈训练配置 - 12v12 Magent2环境，8GB显存优化
# 针对RTX 4060 Ti (8GB) 优化

# 环境配置
env:
  name: "magent2:battle_v4"
  map_size: 20  # 12v12使用20x20地图
  max_cycles: 100  # Episode最大步数
  minimap_mode: false

# Agent配置
agent:
  type: "smpe"  # SMPE² Agent
  obs_dim: 845  # Magent2观测维度
  action_dim: 21
  agent_id_dim: 16  # Agent ID嵌入维度（减小以节省显存）
  
  # 基础PPO配置（减小网络规模）
  encoder:
    type: "networks/mlp"
    params:
      in_dim: 845
      hidden_dims: [128, 64]  # 减小网络规模
      use_layer_norm: true
      dropout: 0.0
  
  policy_head:
    type: "policy_heads/discrete"
    params:
      hidden_dims: [32]  # 减小策略头规模
  
  value_head:
    type: "value_heads/mlp"
    params:
      hidden_dims: [32]  # 减小价值头规模
  
  optimizer:
    type: "optimizers/adam"
    params:
      lr: 3e-4
      scheduler: "linear"
      warmup_steps: 200
      total_steps: 20000
      weight_decay: 0.0
      max_grad_norm: 0.5
  
  # SMPE²模块配置
  use_vae: true  # 启用VAE状态建模
  use_filter: true  # 启用Filter过滤
  use_intrinsic: true  # 启用SimHash内在奖励
  
  # VAE配置（减小模型规模）
  state_dim: 2000  # Magent2环境提供的全局状态维度（20*20*5=2000）
  n_agents: 24  # 智能体数量（12 vs 12）
  vae:
    embedding_shape: 8  # 减小潜在变量维度（从16到8）
    encoder_hidden_dims: [32, 16]  # 减小VAE编码器
    decoder_hidden_dims: [16, 32]  # 减小VAE解码器
    beta_kl: 1e-3  # KL散度权重
    rec_coef: 1.0  # 观测重构权重
    action_rec_coef: 1.0  # 动作重构权重
  
  # Filter配置
  filter:
    num_filters: 4  # 减少过滤器数量（从8到4）
    use_gumbel: false  # 使用Sigmoid而非Gumbel
    temperature: 0.5
    tau: 0.01  # 目标网络软更新系数
  
  # SimHash内在奖励配置
  intrinsic_reward:
    hash_bits: 256  # 减少hash位数（从512到256）
    bucket_size: 32768  # 减小bucket大小
    r_max: 0.2
    normalize: true

# 训练配置（减小batch size和rollout长度）
training:
  num_updates: 2000  # 训练更新次数
  max_steps_per_episode: 100  # Episode最大步数
  num_epochs: 2  # 减少训练轮数
  batch_size: 128  # 减小batch size（从512到128）
  clip_coef: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  vf_clip_param: 10.0  # 价值函数裁剪参数
  gamma: 0.99
  gae_lambda: 0.95
  
  # 梯度累积配置（在显存受限时使用）
  gradient_accumulation_steps: 4  # 累积4个batch，等效batch_size=512
  
  # SMPE²特定配置
  vae_update_freq: 100  # VAE更新频率
  vae_epochs: 1  # VAE训练轮数
  vae_batch_size: 64  # VAE训练时使用更小的batch size
  filter_update_freq: 2  # Filter更新频率
  filter_epochs: 1  # Filter训练轮数
  
  # 组合奖励权重
  lambda_extrinsic: 1.0  # 外部奖励权重
  lambda_intrinsic: 0.1  # 内在奖励权重
  lambda_vae_reward: 0.05  # VAE奖励权重
  
  # 自博弈参数
  self_play_update_freq: 10
  self_play_mode: "pool"
  use_policy_pool: true
  policy_pool_size: 15
  opponent_pool_strategy: "pfsp"
  
  # 评估和保存
  eval_freq: 50
  save_freq: 100
  log_freq: 10

# 其他配置
seed: 42
device: "cuda"

# 数据保存配置
data_saving:
  enabled: true
  format: "json"
  output_dir: "training_data/smpe_magent2_selfplay_12v12_8gb"
  save_freq: 10

# 跟踪配置
tracking:
  enabled: true
  type: "tensorboard"
  project: "marl-selfplay"

